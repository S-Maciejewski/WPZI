{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0) Just some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import common as cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Simple search engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1) Get acquainted with the below class. There are several TODOs. However, DO NOT complete them now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary:\n",
    "    def __init__(self):\n",
    "        ### keeps unique terms (SORTED!)\n",
    "        self.terms = self.loadTerms(\"terms.txt\")\n",
    "        self.idfs = [] ### IDF coefficients\n",
    "        self.corM = [] ### a correlation matrix\n",
    "\n",
    "    ### load terms\n",
    "    def loadTerms(self, fileName):\n",
    "        file = open(fileName,'r', encoding='utf-8-sig')\n",
    "        k = [self.proc(s) for s in file.readlines()]\n",
    "        k.sort()\n",
    "        return k\n",
    "\n",
    "    ### ignore it\n",
    "    def proc(self, s):\n",
    "        if s[-1] == '\\n': return s[:-1]\n",
    "        else: return s\n",
    "    \n",
    "    ### TODO (DO NOT FINISH THIS METHOD YET. YOU WILL BE ASKED FOR IT LATER) \n",
    "    def computeIDFs(self, documents):\n",
    "        docs = len(documents)\n",
    "        for term in self.terms:\n",
    "            self.idfs.append(math.log10(docs / len(list(filter(lambda doc: term in doc.tokens, documents)))))\n",
    "    \n",
    "    ### TODO (DO NOT FINISH THIS METHOD YET. YOU WILL BE ASKED FOR IT LATER) \n",
    "    def computeCorM(self, documents):\n",
    "        def normMatrix(mat):\n",
    "            normalized = []\n",
    "            for x in mat:\n",
    "                normalized.append(x / np.linalg.norm(x))\n",
    "            return np.array(normalized)\n",
    "        \n",
    "        self.corM = []\n",
    "        for i in range(len(self.terms)):\n",
    "            self.corM.append([])\n",
    "            for doc in documents:\n",
    "                self.corM[i].append(doc.tokens.count(self.terms[i]))\n",
    "        self.corM = normMatrix(np.array(self.corM ))\n",
    "        self.corM = np.dot(self.corM, self.corM.T)\n",
    "\n",
    "        for i in range(len(self.corM)):\n",
    "            self.corM[i][i] = -1;\n",
    "        \n",
    "\n",
    "### SOME DEBUG\n",
    "dictionary = Dictionary()\n",
    "# print(dictionary.terms[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2) Load files: here we load some example collection of documents. RAW_DOCUMENTS = just strings. Check if the documents are loaded correctly (e.g., print RAW_DOCUMENTS[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "David W. Aha:  Machine Learning Page\n",
      " Machine Learning Resources. Suggestions welcome. ... (WizRule); ZDM Scientific\n",
      " Ltd. Conference Announcements. Courses on Machine Learning. Data Repositories. ... \n",
      " Description: Comprehensive machine learning resources from Applications to Tutorials.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RAW_DOCUMENTS = cm.loadDocuments(\"documents.txt\")\n",
    "### SOME DEBUG\n",
    "print(RAW_DOCUMENTS[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['david', 'aha', 'machine', 'learning', 'page', 'machine', 'learning', 'resource', 'suggestion', 'welcome', 'wizrule', 'zdm', 'scientific', 'ltd', 'conference', 'announcement', 'course', 'machine', 'learning', 'data', 'repository', 'description', 'comprehensive', 'machine', 'learning', 'resource', 'from', 'application', 'tutorials']\n"
     ]
    }
   ],
   "source": [
    "### SOME DEBUG, JUST RUN; check if (a) common.py is imported correctly and (b) \n",
    "### tokens are correctly derived from some document (e.g., RAW_DOCUMENTS[0])\n",
    "print(cm.simpleTextProcessing(RAW_DOCUMENTS[0], re))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3) Get acquainted with the below class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, doc_id, raw_document, dictionary):\n",
    "        self.doc_id = doc_id ### DOC ID, simply 0,1,2,3....\n",
    "        self.raw_document = raw_document ### raw data, i.e., string data\n",
    "        self.dictionary = dictionary # reference to the dictionary\n",
    "        \n",
    "        ### DOCUMENT REPRESENTATIONS\n",
    "        self.tokens = cm.simpleTextProcessing(raw_document, re) ### get terms\n",
    "        self.bow = [] # Bag Of Words (BOW - number of term occurences)\n",
    "        self.bowDict = []\n",
    "        self.tf = [] # TF representation\n",
    "        self.tf_idf = [] # TF-IDF representation\n",
    "\n",
    "    ### TODO - complete this method; it should compute a BOW representation\n",
    "    def computeBOW_Representation(self):\n",
    "#         for i in range(len(self.tokens)):\n",
    "#             self.bow.append(0)\n",
    "        self.bowDict = dict.fromkeys(self.tokens)\n",
    "        for key in self.bowDict:\n",
    "            self.bowDict[key] = 0\n",
    "        for token in self.tokens:\n",
    "            self.bowDict[token] += 1\n",
    "            \n",
    "        for i in range(len(self.dictionary.terms)):\n",
    "            occurences = 0\n",
    "            for token in self.tokens:\n",
    "                if token == self.dictionary.terms[i]:\n",
    "                    occurences += 1\n",
    "            self.bow.append(occurences)\n",
    "                \n",
    "#         print(self.bow)\n",
    "        \n",
    "    \n",
    "    ### TODO - complete this method; it should compute a TF representation\n",
    "    def computeTF_Representation(self):\n",
    "#         self.tf = self.bowDict.copy()\n",
    "#         tokenAmount = len(self.tokens)\n",
    "#         for key in self.tf:\n",
    "#             self.tf[key] = self.tf[key] / tokenAmount\n",
    "            \n",
    "        self.tf = []\n",
    "        for term in self.dictionary.terms:\n",
    "            self.tf.append(self.tokens.count(term))\n",
    "        maxTermAmount = max(self.tf)\n",
    "        for i in range(len(self.tf)):\n",
    "            self.tf[i] = self.tf[i] / maxTermAmount\n",
    "#         print(self.tf)\n",
    "    \n",
    "    ### TODO - complete this method; it should compute a TFxIDF representation \n",
    "    ### (important: it should not be run before dictionary.idfs are not computed!)\n",
    "    def computeTF_IDF_Representation(self):\n",
    "        self.tf_idf = []\n",
    "#         print('Len comparision - self.tf and dictionary.idfs', len(self.tf), ' - ', len(self.dictionary.idfs))\n",
    "        for i in range(len(self.dictionary.terms)):\n",
    "            self.tf_idf.append(self.tf[i] * self.dictionary.idfs[i])\n",
    "    \n",
    "    def computeRepresentations(self):\n",
    "        self.computeBOW_Representation()\n",
    "        self.computeTF_Representation()\n",
    "        self.computeTF_IDF_Representation()\n",
    "    \n",
    "documents = [Document(i, RAW_DOCUMENTS[i], dictionary) for i in range(len(RAW_DOCUMENTS))]\n",
    "    \n",
    "for doc in documents:\n",
    "    doc.computeBOW_Representation()\n",
    "    doc.computeTF_Representation()\n",
    "    doc.computeTF_IDF_Representation()\n",
    "    \n",
    "# doc = Document(0, RAW_DOCUMENTS[0], dictionary)\n",
    "# print(doc.tokens)\n",
    "# doc.computeBOW_Representation()\n",
    "# doc.computeTF_Representation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4) Compute IDFs here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['working', 1.9444826721501687], ['www', 1.9444826721501687], ['york', 1.9444826721501687], ['young', 1.9444826721501687], ['zdm', 1.9444826721501687]]\n",
      "[['learning', 0.009984220906600923], ['machine', 0.009984220906600923], ['the', 0.1962946451439682], ['and', 0.3212333817522682], ['description', 0.3881801713828814]]\n"
     ]
    }
   ],
   "source": [
    "### TODO COMPUTE IDFS HERE (FINISH THE PROPER METHOD OF THE DICTIONARY CLASS - DO NOT FORGET TO RE-RUN THE CELL)\n",
    "dictionary.computeIDFs(documents)\n",
    "\n",
    "### SOME DEBUG\n",
    "res = [[dictionary.terms[i], dictionary.idfs[i]] for i in range(len(dictionary.terms))]\n",
    "res.sort(key = lambda x: x[1])\n",
    "# LEAST COMMON WORDS - HIGH IDF\n",
    "print(res[-5:])\n",
    "# MOST COMMON WORDS - LOW IDF\n",
    "print(res[:5])\n",
    "# print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5) Compute the document representations (for each document run computeRepresentations())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in documents: d.computeRepresentations()\n",
    "### SOME DEBUG (you should see some 4s - which terms are these?)\n",
    "# print(documents[0].bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.6) Finish the below method. It should compute and return a cosine similarity (v1 and v2 are two vectors - tf-idf representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO \n",
    "def getSimilarity(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.7) Run the below script for different queries. getTopResults seeks for documents being the most similar/relevant to the query. Do you find the results satisfactory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"machine learning\"\n",
    "# query = \"academic research\"\n",
    "#query = \"international conference\"\n",
    "query = \"international conference washington\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANK = 1 WITH SIMILARITY = 0.4742630764397148 | DOC ID = 76\n",
      "ICML 2003\n",
      " The Twentieth International Conference on Machine Learning (ICML-2003).\n",
      " August 21-24, 2003 Washington, DC USA. The Twentieth International ... \n",
      "\n",
      "\n",
      "RANK = 2 WITH SIMILARITY = 0.4212440556739864 | DOC ID = 86\n",
      "International Conferences on Machine Learning and Applications\n",
      " The 2002 International Conferences on Machine Learning and Applications The\n",
      " 2003 International Conferences on Machine Learning and Applications. \n",
      "\n",
      "\n",
      "RANK = 3 WITH SIMILARITY = 0.28379569678898087 | DOC ID = 19\n",
      "1998 International Machine Learning Conference\n",
      " (larger version of cover). The Fifteenth International Conference on Machine Learning.\n",
      " The on-line schedule for ICML-98 contains links to many of the papers. ... \n",
      " Description: The Fifteenth International Conference on Machine Learning. July 24-27, 1998 in Madison, Wisconsin.\n",
      "\n",
      "\n",
      "RANK = 4 WITH SIMILARITY = 0.2000454110462506 | DOC ID = 41\n",
      "ICML-2000\n",
      " Seventeenth International Conference on Machine Learning. Stanford University. ... Tutorials\n",
      " on Commercial Applications of Machine Learning and Data Mining [new]. ... \n",
      " Description: Seventeenth International Conference on Machine Learning. Stanford University June 29-July 2, 2000.\n",
      "\n",
      "\n",
      "RANK = 5 WITH SIMILARITY = 0.19560315498393196 | DOC ID = 39\n",
      "ICML2002 - Sydney\n",
      " The Nineteenth International Conference on Machine Learning (ICML-2002). ... Previous\n",
      " meetings on machine learning: ICML-2001, ICML-2000, ICML-99 , ICML-98. ... \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def getTopResults(query, documents, dictionary, similarity, top = 5):\n",
    "    qd = Document(-1, query, dictionary)\n",
    "    qd.computeRepresentations()\n",
    "    ranks = [[d, getSimilarity(d.tf_idf, qd.tf_idf)] for d in documents]\n",
    "    ranks.sort(key=lambda x: -x[1])\n",
    "    for i in range(top):\n",
    "        print(\"RANK = \" + str(i+1) + \" WITH SIMILARITY = \" + str(ranks[i][1]) + \" | DOC ID = \" + str(ranks[i][0].doc_id))\n",
    "        print(ranks[i][0].raw_document)\n",
    "        print(\"\")\n",
    "\n",
    "getTopResults(query, documents, dictionary, getSimilarity, top = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Query expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) Correlation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.1) Finish dictionary.computeCorM method (see class Dictionary). It should generate a correlation matrix (correlation between terms).\n",
    "\n",
    "IMPORTANT: although corM[ i ][ i ] (for each i) should be 1.0, set it to -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO - COMPLETE THE computeCorM METHOD (see one of the first cells)\n",
    "dictionary.computeCorM(documents)\n",
    "# print(dictionary.corM.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1.2) Finish the below method. For each term in the query (you must parse the query, see getTopResults() method), find another term which is the most correlated with the input term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suggestion for  learning  is  machine  with  0.9700552531915929  correlation\n"
     ]
    }
   ],
   "source": [
    "# query = \"machine\"\n",
    "# query = \"algorithm\"\n",
    "query = \"learning\"\n",
    "# query = \"conference\"\n",
    "# query = \"research\"\n",
    "# query = \"concept\"\n",
    "\n",
    "def suggestKeywords(query, dictionary):\n",
    "    termIndex = dictionary.terms.index(query)\n",
    "    corM = dictionary.corM.tolist()\n",
    "    suggestedTermIndex = corM[termIndex].index(max(corM[termIndex]))\n",
    "    print(\"Suggestion for \", dictionary.terms[termIndex], ' is ', dictionary.terms[suggestedTermIndex],\n",
    "         ' with ', corM[termIndex][suggestedTermIndex], ' correlation')\n",
    "    \n",
    "suggestKeywords(query, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2) Rocchio algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\overrightarrow{q_{m}} = \\alpha \\overrightarrow{q} + \\left(\\beta \\cdot \\dfrac{1}{|D_{r}|} \\sum_{\\overrightarrow{D_j} \\in D_{r}} \\overrightarrow{D_j} \\right) - \\left(\\gamma \\cdot \\dfrac{1}{|D_{nr}|} \\sum_{\\overrightarrow{D_j} \\in D_{nr}} \\overrightarrow{D_j} \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.1) Firstly, run the below code. Observe the results. Assume that we do not like the first and the second result (Docs 63 and 77). However, assume that docs 29 and 36 are satisfactory. Now, modfify the method. It should alter the query vector, according to Rocchio algorithm. Check the result for the above considered scenario (relevant docs = 29 and 36; not relevant = 63 and 77). Check the results for different values of alpha, beta, and gamma coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANK = 1 WITH SIMILARITY = 0.684003251690511 | DOC ID = 29\n",
      "Machine Learning\n",
      " 6.858/18.428: Machine Learning. ... This course deals with the following topics:\n",
      " Formal models of machine learning; Learning concepts from examples; ... \n",
      "\n",
      "\n",
      "RANK = 2 WITH SIMILARITY = 0.619753799621289 | DOC ID = 36\n",
      "Machine Learning Group\n",
      " [ Bristol CS | Index | Research | ML group | Student projects ] Machine\n",
      " Learning Research Group. Overview. The Machine Learning Research ... \n",
      " Description: Research on higher-order concept learning, inductive logic programming, multi-agent learning systems,...\n",
      "\n",
      "\n",
      "RANK = 3 WITH SIMILARITY = 0.18835088020430527 | DOC ID = 13\n",
      "UTCS Machine Learning Research Group\n",
      " Machine learning is the study of adaptive computational systems that\n",
      " improve their performance with experience. The UT Machine Learning ... \n",
      " Description: Research on General Inductive Learning, Inductive Logic Programming, Natural Language Learning, Qualitati...\n",
      "\n",
      "\n",
      "RANK = 4 WITH SIMILARITY = 0.1553643079974389 | DOC ID = 81\n",
      "Oxford University Machine Learning Group\n",
      " Machine Learning at the Computing Laboratory. ... Logic Programming and\n",
      " Learning and Intelligent Systems. Other Machine Learning Groups. ... \n",
      "\n",
      "\n",
      "RANK = 5 WITH SIMILARITY = 0.14796618056694452 | DOC ID = 68\n",
      "CWIS: Machine Learning Research Group\n",
      " ... Click here to go to the new webpage of the ML group. Machine Learning research\n",
      " group. Current research Publications Conferences relevant to us. ... \n",
      "\n",
      "\n",
      "RANK = 6 WITH SIMILARITY = 0.14309160121195014 | DOC ID = 73\n",
      "Machine Learning/Genetic Algorithm group - Dipartimento di ... \n",
      " ... The Machine Learning group at the Dipartimento di Informatica, Universita di Torino\n",
      " is part of a larger research group on Artificial Intelligence, and is ... \n",
      " Description: Research on learning first-order classification rules, first-order concept descriptions, genetic algorith...\n",
      "\n",
      "\n",
      "RANK = 7 WITH SIMILARITY = 0.1395535832287037 | DOC ID = 18\n",
      "Home Page of the UW-Madison Machine Learning Research Group\n",
      " This page contains relevant information about, and for, the members of the Machine\n",
      " Learning Research Group (MLRG) at the University of Wisconsin - Madison. ... \n",
      " Description: Research on information retrieval and extraction, bioinformatics, connectionist models, hybrid systems.\n",
      "\n",
      "\n",
      "RANK = 8 WITH SIMILARITY = 0.10565627218574637 | DOC ID = 14\n",
      "Machine Learning Research Software\n",
      " Machine Learning Research Software. We have compiled a group of Common\n",
      " Lisp files for various inductive classification algorithms. ... \n",
      "\n",
      "\n",
      "RANK = 9 WITH SIMILARITY = 0.10239500315322836 | DOC ID = 7\n",
      "15-681 and 15-781 Machine Learning\n",
      " Machine Learning, 15:681 and 15:781, Fall 1998. ... This course covers the theory\n",
      " and practice of machine learning from a variety of perspectives. ... \n",
      "\n",
      "\n",
      "RANK = 10 WITH SIMILARITY = 0.09687236592423544 | DOC ID = 17\n",
      "Machine Learning Courses\n",
      " Index of Machine Learning Courses. Machine Learning,Altay Guvenir, Bilkent University,\n",
      " Turkey. Machine Learning, Tony Martinez, Brigham Young University. ... \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def getTopResults_Rocchio(query, \n",
    "                          documents, \n",
    "                          dictionary, \n",
    "                          similarity, \n",
    "                          rel_docs = [29, 36],\n",
    "                          nrel_docs = [63, 77],\n",
    "                          alpha = 0.5,\n",
    "                          beta = 0.3,\n",
    "                          gamma = 0.2,\n",
    "                          top = 10):\n",
    "    qd = Document(-1, query, dictionary)\n",
    "    qd.computeRepresentations()\n",
    "    ##### TODO: MODIFY qd.tf_idf HERE\n",
    "    \n",
    "    alphaQ = map(lambda q: q * alpha, qd.tf_idf)\n",
    "    \n",
    "    relTfIdf = map(lambda x: documents[x].tf_idf, rel_docs)\n",
    "    betaD = map(lambda x: x * beta,\n",
    "                reduce(lambda x, y: list(\n",
    "                    map(lambda a, b: a + b, x, y)), relTfIdf))\n",
    "    \n",
    "    nrelTfIdf = map(lambda x: documents[x].tf_idf, nrel_docs)\n",
    "    gammaD = map(lambda x: x * gamma, \n",
    "                 reduce(lambda x, y: list(\n",
    "                     map(lambda a, b: a + b, x, y)), nrelTfIdf))\n",
    "    \n",
    "    qd.tf_idf = list(map(lambda x, y, z: x + y - z, alphaQ, betaD, gammaD))\n",
    "    \n",
    "    #####\n",
    "    ranks = [[d, getSimilarity(d.tf_idf, qd.tf_idf)] for d in documents]\n",
    "    ranks.sort(key=lambda x: -x[1])\n",
    "    for i in range(top):\n",
    "        print(\"RANK = \" + str(i+1) + \" WITH SIMILARITY = \" + str(ranks[i][1]) + \" | DOC ID = \" + str(ranks[i][0].doc_id))\n",
    "        print(ranks[i][0].raw_document)\n",
    "        print(\"\")\n",
    "\n",
    "getTopResults_Rocchio(\"machine learning\", documents, dictionary, getSimilarity, top = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3) WordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.1) Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.nltk.org/install.html\n",
    "\n",
    "import nltk \n",
    "\n",
    "nltk.download()\n",
    "\n",
    "https://www.nltk.org/data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/seba/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition: synset = (from wiki) (information science) A set of one or more synonyms that are interchangeable in some context without changing the truth value of the proposition in which they are embedded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.2) Display sysents for \"machine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('machine.n.01'),\n",
       " Synset('machine.n.02'),\n",
       " Synset('machine.n.03'),\n",
       " Synset('machine.n.04'),\n",
       " Synset('machine.n.05'),\n",
       " Synset('car.n.01'),\n",
       " Synset('machine.v.01'),\n",
       " Synset('machine.v.02')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('machine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.3) Display all definitions (.definition()) for synsets (machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine is:  any mechanical or electrical device that transmits or modifies energy to perform or assist in the performance of human tasks\n",
      "machine is:  an efficient person\n",
      "machine is:  an intricate organization that accomplishes its goals efficiently\n",
      "machine is:  a device for overcoming resistance at one point by applying force at some other point\n",
      "machine is:  a group that controls the activities of a political party\n",
      "machine is:  a motor vehicle with four wheels; usually propelled by an internal combustion engine\n",
      "machine is:  turn, shape, mold, or otherwise finish by machinery\n",
      "machine is:  make by machinery\n"
     ]
    }
   ],
   "source": [
    "for synset in wn.synsets('machine'):\n",
    "    print('machine is: ', synset.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.4) For each synset (machine), display its hypernym (a word with a broad meaning constituting a category into which words with more specific meanings fall; a superordinate. For example, colour is a hypernym of red)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device.n.01\n",
      "person.n.01\n",
      "organization.n.01\n",
      "mechanical_device.n.01\n",
      "organization.n.01\n",
      "motor_vehicle.n.01\n",
      "shape.v.02\n",
      "produce.v.02\n"
     ]
    }
   ],
   "source": [
    "for synset in wn.synsets('machine'):\n",
    "#     print(synset.hypernyms())\n",
    "    for hypernym in synset.hypernyms():\n",
    "       print(hypernym.name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "See: http://www.nltk.org/howto/wordnet.html\n",
    "for more examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
